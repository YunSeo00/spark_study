{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, desc"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 미국 항공사 데이터 예제"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**데이터 불러오기**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/24 23:49:59 WARN Utils: Your hostname, choeyunseoui-MacBookAir.local resolves to a loopback address: 127.0.0.1; using 192.168.0.17 instead (on interface en0)\n",
      "23/03/24 23:49:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/24 23:50:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# SparkSession 생성\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"SparkSQLExampleApp\")\n",
    "         .getOrCreate())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "schema = \"date STRING, delay INT, distance INT, origin STRING, destination STRING\"\n",
    "\n",
    "df = (spark.read.format(\"csv\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .load(\"departuredelays.csv\", schema=schema))\n",
    "\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 예제1. 비행거리가 1000마일 이상인 항공편\n",
    "spark.sql(\"\"\"SELECT distance, origin, destination\n",
    "             FROM us_delay_flights_tbl WHERE distance > 1000\n",
    "             ORDER BY distance DESC\"\"\").show(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=======>                                                   (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 예제1. 동일한 계산을 데이터 프레임으로도 수행할 수 있다.\n",
    "(df.select(\"distance\", \"origin\", \"destination\")\n",
    " .where(col(\"distance\") > 1000)\n",
    " .orderBy(desc(\"distance\"))).show(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|02190925| 1638|   SFO|        ORD|\n",
      "|01031755|  396|   SFO|        ORD|\n",
      "|01022330|  326|   SFO|        ORD|\n",
      "|01051205|  320|   SFO|        ORD|\n",
      "|01190925|  297|   SFO|        ORD|\n",
      "|02171115|  296|   SFO|        ORD|\n",
      "|01071040|  279|   SFO|        ORD|\n",
      "|01051550|  274|   SFO|        ORD|\n",
      "|03120730|  266|   SFO|        ORD|\n",
      "|01261104|  258|   SFO|        ORD|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 샌프란시스코(SFO)와 시카고(ORD)간 2시간 이상 지연된 항공편\n",
    "spark.sql(\"\"\"SELECT date, delay, origin, destination\n",
    "             FROM us_delay_flights_tbl\n",
    "             WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD'\n",
    "             ORDER by delay DESC\"\"\").show(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# date data type 변경\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "add_df = (df.withColumn(\"newdate\", to_timestamp(col(\"date\"), \"MMddhhmm\"))\n",
    "             .withColumn(\"month\", month(col(\"newdate\")))\n",
    "             .withColumn(\"day\", dayofmonth(col(\"newdate\"))))\n",
    "\n",
    "add_df.createOrReplaceTempView(\"us_delay_flights_tbl_add_ver\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+-------------------+-----+---+\n",
      "|    date|delay|distance|origin|destination|            newdate|month|day|\n",
      "+--------+-----+--------+------+-----------+-------------------+-----+---+\n",
      "|01011245|    6|     602|   ABE|        ATL|1970-01-01 00:45:00|    1|  1|\n",
      "|01020600|   -8|     369|   ABE|        DTW|1970-01-02 06:00:00|    1|  2|\n",
      "|01021245|   -2|     602|   ABE|        ATL|1970-01-02 00:45:00|    1|  2|\n",
      "|01020605|   -4|     602|   ABE|        ATL|1970-01-02 06:05:00|    1|  2|\n",
      "|01031245|   -4|     602|   ABE|        ATL|1970-01-03 00:45:00|    1|  3|\n",
      "+--------+-----+--------+------+-----------+-------------------+-----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "add_df.show(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "p.91 의 연습문제\n",
    "- date 칼럼을 읽을 수 있는 포맷으로 변경하고, 가장 흔하게 지연이 발생한 날짜나 달을 찾아보기"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:====================================>                      (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------+------------------+\n",
      "|month|day|count(1)|        avg(delay)|\n",
      "+-----+---+--------+------------------+\n",
      "|    2| 19|      12|             138.0|\n",
      "|    3| 12|      13| 67.15384615384616|\n",
      "|    2| 17|      12|             51.25|\n",
      "|    1|  5|      10|              44.5|\n",
      "|    3| 26|      13| 43.30769230769231|\n",
      "|    1|  2|      12|             39.25|\n",
      "|    1| 30|      13| 36.15384615384615|\n",
      "|    3|  5|      12|35.916666666666664|\n",
      "|    2|  8|       8|            33.125|\n",
      "|    3| 17|      13|31.692307692307693|\n",
      "+-----+---+--------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT month, day, count(*), avg(delay)\n",
    "             FROM us_delay_flights_tbl_add_ver\n",
    "             WHERE ORIGIN = 'SFO' AND DESTINATION = 'ORD'\n",
    "             GROUP BY month, day\n",
    "             ORDER by avg(delay) DESC, month , day \"\"\").show(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 지연 정도를 나타내는 Flight_Delays\n",
    "spark.sql(\"\"\"SELECT delay, origin, destination,\n",
    "             CASE\n",
    "                WHEN delay > 360 THEN 'Very Long Delays'\n",
    "                WHEN delay >= 120 AND delay <= 360 THEN 'Long Delays'\n",
    "                WHEN delay >= 60 AND delay < 120 THEN 'Short Delays'\n",
    "                WHEN delay > 0 AND delay < 60 THEN 'Tolerable Delays'\n",
    "                WHEN delay = 0 THEN 'No Delays'\n",
    "                ELSE 'Early'\n",
    "            END AS Flight_Delays\n",
    "            FROM us_delay_flights_tbl\n",
    "            ORDER BY origin, delay DESC\"\"\").show(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SQL과 테이블 뷰"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 베이스 생성\n",
    "spark.sql(\"CREATE DATABASE learn_spark_db\")\n",
    "spark.sql(\"USE learn_spark_db\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/25 00:00:36 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/03/25 00:00:36 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/03/25 00:00:38 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 관리형 테이블 생성\n",
    "flights_df = spark.read.csv(\"departuredelays.csv\", schema=schema)\n",
    "flights_df.write.saveAsTable(\"managed_us_delay_flights_tbl\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 비관리형 테이블 생성\n",
    "# 1. sql\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE us_delay_flights_tbl (\n",
    "    date STRING,\n",
    "    delay INT,\n",
    "    distance INT,\n",
    "    origin STRING,\n",
    "    destination STRING\n",
    "    )\n",
    "    USING csv OPTIONS (PATH 'departuredelays.csv')\n",
    "    \"\"\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/25 00:03:13 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/03/25 00:03:13 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/25 00:03:14 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 2. 데이터 프레임 API\n",
    "(flights_df\n",
    " .write\n",
    " .option(\"path\", \"/tmp/data/us_flights_delay\")\n",
    " .saveAsTable(\"us_delay_flights_tbl2\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# 항공사 데이터에서 전역/일반 임시 뷰 생성\n",
    "df_sfo = spark.sql(\"\"\"SELECT date, delay, origin, destination\n",
    "                      FROM us_delay_flights_tbl\n",
    "                      WHERE origin = 'SFO'\"\"\")\n",
    "df_jfk = spark.sql(\"\"\"SELECT date, delay, origin, destination\n",
    "                      FROM us_delay_flights_tbl\n",
    "                      WHERE origin = 'JFK'\"\"\")\n",
    "\n",
    "# create a temporary and global temporary view\n",
    "df_sfo.createOrReplaceGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "df_jfk.createOrReplaceTempView(\"us_origin_airport_JFK_tmp_view\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|01011250|   55|   SFO|        JFK|\n",
      "|01012230|    0|   SFO|        JFK|\n",
      "|01010705|   -7|   SFO|        JFK|\n",
      "|01010620|   -3|   SFO|        MIA|\n",
      "|01010915|   -3|   SFO|        LAX|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|01011250|   55|   SFO|        JFK|\n",
      "|01012230|    0|   SFO|        JFK|\n",
      "|01010705|   -7|   SFO|        JFK|\n",
      "|01010620|   -3|   SFO|        MIA|\n",
      "|01010915|   -3|   SFO|        LAX|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 전역 임시 뷰 접근\n",
    "# 방법1\n",
    "spark.read.table(\"global_temp.us_origin_airport_SFO_global_tmp_view\").show(5)\n",
    "# 방법2\n",
    "spark.sql(\"SELECT * FROM global_temp.us_origin_airport_SFO_global_tmp_view\").show(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|01010900|   14|   JFK|        LAX|\n",
      "|01011200|   -3|   JFK|        LAX|\n",
      "|01011900|    2|   JFK|        LAX|\n",
      "|01011700|   11|   JFK|        LAS|\n",
      "|01010800|   -1|   JFK|        SFO|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|01010900|   14|   JFK|        LAX|\n",
      "|01011200|   -3|   JFK|        LAX|\n",
      "|01011900|    2|   JFK|        LAX|\n",
      "|01011700|   11|   JFK|        LAS|\n",
      "|01010800|   -1|   JFK|        SFO|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 일반 임시 뷰 접근\n",
    "# 방법1\n",
    "spark.read.table(\"us_origin_airport_JFK_tmp_view\").show(5)\n",
    "# 방법2\n",
    "spark.sql(\"SELECT * FROM us_origin_airport_JFK_tmp_view\").show(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 뷰 드롭\n",
    "spark.catalog.dropGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "spark.catalog.dropTempView(\"us_origin_airport_JFK_tmp_view\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
