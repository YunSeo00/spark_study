{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/14 17:22:35 WARN Utils: Your hostname, choeyunseoui-MacBookAir.local resolves to a loopback address: 127.0.0.1; using 192.168.0.10 instead (on interface en0)\n",
      "23/05/14 17:22:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/14 17:22:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# SparkSession 생성\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"SparkMllibExampleApp\")\n",
    "         .getOrCreate())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 스파크 설정 변경하기"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 설정 변경 가능 여부 확인하기\n",
    "spark.conf.isModifiable(\"spark.sql.shuffle.partitions\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "'200'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "'5'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 캐싱과 영속화"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- cache()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "df = spark.range(1 * 10000000).toDF(\"id\").withColumn(\"square\", col(\"id\") * col(\"id\") )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.67982292175293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.cache()\n",
    "start = time.time()\n",
    "df.count()\n",
    "end = time.time()\n",
    "print(end - start)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7317070960998535\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df.count()\n",
    "end = time.time()\n",
    "print(end - start)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- persist()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "df = spark.range(1 * 10000000).toDF(\"id\").withColumn(\"square\", col(\"id\") * col(\"id\") )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6994738578796387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.persist(StorageLevel.DISK_ONLY)\n",
    "start = time.time()\n",
    "df.count()\n",
    "end = time.time()\n",
    "print(end - start)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4061131477355957\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df.count()\n",
    "end = time.time()\n",
    "print(end - start)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[id: bigint, square: bigint]"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.unpersist()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Suffle Sort Merge Join"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import random"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") # 자동으로 SMJ 시행"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "states_dict = {0:\"AZ\", 1:\"CO\", 2:\"CA\", 3:\"TX\", 4:\"NY\", 5:\"MI\" }\n",
    "items_dict = {0:\"SKU-0\", 1:\"SKU-1\", 2:\"SKU-2\", 3:\"SKU-3\", 4:\"SKU-4\", 5:\"SKU-5\"}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# usersDF 생성\n",
    "usersDF = spark.range(1 * 10000000).rdd.map(lambda x: (str(x[0]),\n",
    "                                                       \"user_\"+str(x[0]),\n",
    "                                                       \"user_\"+str(x[0])+\"@databricks.com\",\n",
    "                                                       states_dict[random.choice(range(6))])\n",
    "                                            ).toDF([\"uid\", \"login\", \"email\", \"user_state\"])\n",
    "#usersDF.show(6)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# ordersDF 생성\n",
    "ordersDF = spark.range(1 * 10000000).rdd.map(lambda x: (x[0],\n",
    "                                                        x[0],\n",
    "                                                        str(random.choice(range(10001))),\n",
    "                                                        10 * x[0] * 0.2,\n",
    "                                                        states_dict[random.choice(range(6))],\n",
    "                                                        items_dict[random.choice(range(6))])\n",
    "                                             ).toDF([\"transaction_id\", \"quantity\", \"users_id\", \"amount\", \"state\", \"items\"])\n",
    "#ordersDF.show(6)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# join\n",
    "usersOrdersDF = ordersDF.join(usersDF, ordersDF.users_id == usersDF.uid)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+--------+-----+-----+----+---------+--------------------+----------+\n",
      "|transaction_id|quantity|users_id|  amount|state|items| uid|    login|               email|user_state|\n",
      "+--------------+--------+--------+--------+-----+-----+----+---------+--------------------+----------+\n",
      "|         11549|   11549|    1008| 23098.0|   NY|SKU-4|1008|user_1008|user_1008@databri...|        NY|\n",
      "|         14093|   14093|    1008| 28186.0|   CA|SKU-2|1008|user_1008|user_1008@databri...|        NY|\n",
      "|         23123|   23123|    1008| 46246.0|   MI|SKU-2|1008|user_1008|user_1008@databri...|        NY|\n",
      "|         29264|   29264|    1008| 58528.0|   CA|SKU-3|1008|user_1008|user_1008@databri...|        NY|\n",
      "|         47249|   47249|    1008| 94498.0|   CO|SKU-4|1008|user_1008|user_1008@databri...|        NY|\n",
      "|         67306|   67306|    1008|134612.0|   MI|SKU-0|1008|user_1008|user_1008@databri...|        NY|\n",
      "|         78767|   78767|    1008|157534.0|   AZ|SKU-0|1008|user_1008|user_1008@databri...|        NY|\n",
      "|         92225|   92225|    1008|184450.0|   CO|SKU-0|1008|user_1008|user_1008@databri...|        NY|\n",
      "|        106250|  106250|    1008|212500.0|   AZ|SKU-3|1008|user_1008|user_1008@databri...|        NY|\n",
      "|        115039|  115039|    1008|230078.0|   CA|SKU-4|1008|user_1008|user_1008@databri...|        NY|\n",
      "|        150206|  150206|    1008|300412.0|   AZ|SKU-4|1008|user_1008|user_1008@databri...|        NY|\n",
      "|        178470|  178470|    1008|356940.0|   NY|SKU-1|1008|user_1008|user_1008@databri...|        NY|\n",
      "|        187750|  187750|    1008|375500.0|   CA|SKU-3|1008|user_1008|user_1008@databri...|        NY|\n",
      "|        204666|  204666|    1008|409332.0|   MI|SKU-4|1008|user_1008|user_1008@databri...|        NY|\n",
      "|        215515|  215515|    1008|431030.0|   CA|SKU-1|1008|user_1008|user_1008@databri...|        NY|\n",
      "|        221469|  221469|    1008|442938.0|   CA|SKU-4|1008|user_1008|user_1008@databri...|        NY|\n",
      "|        237020|  237020|    1008|474040.0|   NY|SKU-0|1008|user_1008|user_1008@databri...|        NY|\n",
      "|        243844|  243844|    1008|487688.0|   MI|SKU-3|1008|user_1008|user_1008@databri...|        NY|\n",
      "|        243936|  243936|    1008|487872.0|   NY|SKU-0|1008|user_1008|user_1008@databri...|        NY|\n",
      "|        245036|  245036|    1008|490072.0|   CO|SKU-2|1008|user_1008|user_1008@databri...|        NY|\n",
      "+--------------+--------+--------+--------+-----+-----+----+---------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersOrdersDF.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [users_id#16], [uid#4], Inner\n",
      "   :- Sort [users_id#16 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(users_id#16, 200), ENSURE_REQUIREMENTS, [plan_id=148]\n",
      "   :     +- Filter isnotnull(users_id#16)\n",
      "   :        +- Scan ExistingRDD[transaction_id#14L,quantity#15L,users_id#16,amount#17,state#18,items#19]\n",
      "   +- Sort [uid#4 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(uid#4, 200), ENSURE_REQUIREMENTS, [plan_id=149]\n",
      "         +- Filter isnotnull(uid#4)\n",
      "            +- Scan ExistingRDD[uid#4,login#5,email#6,user_state#7]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersOrdersDF.explain()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### bucketing을 적용한 SMJ"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/14 17:37:56 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:37:56 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/05/14 17:38:00 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:38:00 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:38:00 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/05/14 17:38:02 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:38:02 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:38:02 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/05/14 17:38:03 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:38:03 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:38:03 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/05/14 17:38:05 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:38:05 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:38:05 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/05/14 17:38:06 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:38:06 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:38:06 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/05/14 17:38:07 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:38:07 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:38:07 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/05/14 17:38:08 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:38:08 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:38:08 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/05/14 17:38:09 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/14 17:39:32 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:39:32 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/05/14 17:39:35 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:39:35 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:39:35 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/05/14 17:39:36 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:39:37 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:39:37 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/05/14 17:39:38 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:39:38 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:39:38 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/05/14 17:39:38 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:39:38 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:39:38 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/05/14 17:39:39 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:39:39 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:39:39 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/05/14 17:39:39 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:39:40 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:39:40 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/05/14 17:39:40 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:39:40 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/05/14 17:39:40 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:39:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:39:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:39:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/05/14 17:39:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:39:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/05/14 17:39:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:39:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/05/14 17:39:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:39:42 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:39:42 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:39:42 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/05/14 17:39:42 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:39:42 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/05/14 17:39:42 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:39:43 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:39:43 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/14 17:39:43 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/05/14 17:39:43 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/14 17:40:09 WARN MemoryStore: Not enough space to cache rdd_60_1 in memory! (computed 37.0 MiB so far)\n",
      "23/05/14 17:40:09 WARN MemoryStore: Not enough space to cache rdd_60_6 in memory! (computed 37.0 MiB so far)\n",
      "23/05/14 17:40:09 WARN MemoryStore: Not enough space to cache rdd_60_4 in memory! (computed 37.0 MiB so far)\n",
      "23/05/14 17:40:09 WARN BlockManager: Persisting block rdd_60_6 to disk instead.\n",
      "23/05/14 17:40:09 WARN BlockManager: Persisting block rdd_60_4 to disk instead.\n",
      "23/05/14 17:40:09 WARN BlockManager: Persisting block rdd_60_1 to disk instead.\n",
      "23/05/14 17:40:22 WARN MemoryStore: Not enough space to cache rdd_60_7 in memory! (computed 64.4 MiB so far)\n",
      "23/05/14 17:40:22 WARN MemoryStore: Not enough space to cache rdd_60_0 in memory! (computed 64.4 MiB so far)\n",
      "23/05/14 17:40:22 WARN BlockManager: Persisting block rdd_60_0 to disk instead.\n",
      "23/05/14 17:40:22 WARN BlockManager: Persisting block rdd_60_7 to disk instead.\n",
      "23/05/14 17:40:22 WARN MemoryStore: Not enough space to cache rdd_60_3 in memory! (computed 64.4 MiB so far)\n",
      "23/05/14 17:40:22 WARN BlockManager: Persisting block rdd_60_3 to disk instead.\n",
      "23/05/14 17:40:26 WARN MemoryStore: Not enough space to cache rdd_60_7 in memory! (computed 64.4 MiB so far)\n",
      "23/05/14 17:40:26 WARN MemoryStore: Not enough space to cache rdd_60_3 in memory! (computed 64.4 MiB so far)\n",
      "23/05/14 17:40:27 WARN MemoryStore: Not enough space to cache rdd_60_6 in memory! (computed 18.8 MiB so far)\n",
      "23/05/14 17:40:27 WARN MemoryStore: Not enough space to cache rdd_60_1 in memory! (computed 18.8 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# usersDF bucketing\n",
    "usersDF.orderBy(col(\"uid\").asc()) \\\n",
    "       .write.format(\"parquet\") \\\n",
    "       .bucketBy(8, \"uid\") \\\n",
    "       .mode(\"overwrite\") \\\n",
    "       .saveAsTable(\"UsersTbl\")\n",
    "\n",
    "# ordersDF bucketing\n",
    "ordersDF.orderBy(col(\"users_id\").asc()) \\\n",
    "       .write.format(\"parquet\") \\\n",
    "       .bucketBy(8, \"users_id\") \\\n",
    "       .mode(\"overwrite\") \\\n",
    "       .saveAsTable(\"OrdersTbl\")\n",
    "\n",
    "# table caching\n",
    "spark.sql(\"CACHE TABLE UsersTbl\")\n",
    "spark.sql(\"CACHE TABLE OrdersTbl\")\n",
    "\n",
    "# reread\n",
    "usersBucketDF = spark.table(\"UsersTbl\")\n",
    "ordersBucketDF = spark.table(\"OrdersTbl\")\n",
    "\n",
    "# join\n",
    "joinUsersOrdersBucketDF = ordersBucketDF.join(usersBucketDF, ordersBucketDF.users_id == usersBucketDF.uid)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+-----------+-----+-----+---+------+--------------------+----------+\n",
      "|transaction_id|quantity|users_id|     amount|state|items|uid| login|               email|user_state|\n",
      "+--------------+--------+--------+-----------+-----+-----+---+------+--------------------+----------+\n",
      "|       3764678| 3764678|       1|  7529356.0|   MI|SKU-2|  1|user_1|user_1@databricks...|        CO|\n",
      "|       6264729| 6264729|       1|1.2529458E7|   NY|SKU-5|  1|user_1|user_1@databricks...|        CO|\n",
      "|       3779537| 3779537|       1|  7559074.0|   CO|SKU-3|  1|user_1|user_1@databricks...|        CO|\n",
      "|       6265212| 6265212|       1|1.2530424E7|   CO|SKU-3|  1|user_1|user_1@databricks...|        CO|\n",
      "|       3784657| 3784657|       1|  7569314.0|   NY|SKU-4|  1|user_1|user_1@databricks...|        CO|\n",
      "|       6277990| 6277990|       1| 1.255598E7|   CA|SKU-2|  1|user_1|user_1@databricks...|        CO|\n",
      "|       3784702| 3784702|       1|  7569404.0|   NY|SKU-2|  1|user_1|user_1@databricks...|        CO|\n",
      "|       6281939| 6281939|       1|1.2563878E7|   AZ|SKU-4|  1|user_1|user_1@databricks...|        CO|\n",
      "|       3813014| 3813014|       1|  7626028.0|   TX|SKU-2|  1|user_1|user_1@databricks...|        CO|\n",
      "|       6288459| 6288459|       1|1.2576918E7|   NY|SKU-1|  1|user_1|user_1@databricks...|        CO|\n",
      "|       3813202| 3813202|       1|  7626404.0|   CO|SKU-5|  1|user_1|user_1@databricks...|        CO|\n",
      "|       6296284| 6296284|       1|1.2592568E7|   CA|SKU-5|  1|user_1|user_1@databricks...|        CO|\n",
      "|       3843476| 3843476|       1|  7686952.0|   TX|SKU-2|  1|user_1|user_1@databricks...|        CO|\n",
      "|       6309857| 6309857|       1|1.2619714E7|   NY|SKU-0|  1|user_1|user_1@databricks...|        CO|\n",
      "|       3850052| 3850052|       1|  7700104.0|   CA|SKU-2|  1|user_1|user_1@databricks...|        CO|\n",
      "|       6312461| 6312461|       1|1.2624922E7|   AZ|SKU-1|  1|user_1|user_1@databricks...|        CO|\n",
      "|       3875480| 3875480|       1|  7750960.0|   CO|SKU-0|  1|user_1|user_1@databricks...|        CO|\n",
      "|       6356689| 6356689|       1|1.2713378E7|   TX|SKU-0|  1|user_1|user_1@databricks...|        CO|\n",
      "|       3889081| 3889081|       1|  7778162.0|   CA|SKU-2|  1|user_1|user_1@databricks...|        CO|\n",
      "|       6358520| 6358520|       1| 1.271704E7|   NY|SKU-4|  1|user_1|user_1@databricks...|        CO|\n",
      "+--------------+--------+--------+-----------+-----+-----+---+------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "joinUsersOrdersBucketDF.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=true\n",
      "+- == Final Plan ==\n",
      "   *(3) SortMergeJoin [users_id#268], [uid#129], Inner\n",
      "   :- *(1) Sort [users_id#268 ASC NULLS FIRST], false, 0\n",
      "   :  +- *(1) Filter isnotnull(users_id#268)\n",
      "   :     +- Scan In-memory table OrdersTbl [transaction_id#266L, quantity#267L, users_id#268, amount#269, state#270, items#271], [isnotnull(users_id#268)]\n",
      "   :           +- InMemoryRelation [transaction_id#266L, quantity#267L, users_id#268, amount#269, state#270, items#271], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   :                 +- *(1) ColumnarToRow\n",
      "   :                    +- FileScan parquet default.orderstbl[transaction_id#266L,quantity#267L,users_id#268,amount#269,state#270,items#271] Batched: true, Bucketed: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/yschoi/Library/CloudStorage/Dropbox/yunseo/development/BOA..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<transaction_id:bigint,quantity:bigint,users_id:string,amount:double,state:string,items:str..., SelectedBucketsCount: 8 out of 8\n",
      "   +- *(2) Sort [uid#129 ASC NULLS FIRST], false, 0\n",
      "      +- *(2) Filter isnotnull(uid#129)\n",
      "         +- Scan In-memory table UsersTbl [uid#129, login#130, email#131, user_state#132], [isnotnull(uid#129)]\n",
      "               +- InMemoryRelation [uid#129, login#130, email#131, user_state#132], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                     +- *(1) ColumnarToRow\n",
      "                        +- FileScan parquet default.userstbl[uid#129,login#130,email#131,user_state#132] Batched: true, Bucketed: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/yschoi/Library/CloudStorage/Dropbox/yunseo/development/BOA..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<uid:string,login:string,email:string,user_state:string>, SelectedBucketsCount: 8 out of 8\n",
      "+- == Initial Plan ==\n",
      "   SortMergeJoin [users_id#268], [uid#129], Inner\n",
      "   :- Sort [users_id#268 ASC NULLS FIRST], false, 0\n",
      "   :  +- Filter isnotnull(users_id#268)\n",
      "   :     +- Scan In-memory table OrdersTbl [transaction_id#266L, quantity#267L, users_id#268, amount#269, state#270, items#271], [isnotnull(users_id#268)]\n",
      "   :           +- InMemoryRelation [transaction_id#266L, quantity#267L, users_id#268, amount#269, state#270, items#271], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   :                 +- *(1) ColumnarToRow\n",
      "   :                    +- FileScan parquet default.orderstbl[transaction_id#266L,quantity#267L,users_id#268,amount#269,state#270,items#271] Batched: true, Bucketed: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/yschoi/Library/CloudStorage/Dropbox/yunseo/development/BOA..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<transaction_id:bigint,quantity:bigint,users_id:string,amount:double,state:string,items:str..., SelectedBucketsCount: 8 out of 8\n",
      "   +- Sort [uid#129 ASC NULLS FIRST], false, 0\n",
      "      +- Filter isnotnull(uid#129)\n",
      "         +- Scan In-memory table UsersTbl [uid#129, login#130, email#131, user_state#132], [isnotnull(uid#129)]\n",
      "               +- InMemoryRelation [uid#129, login#130, email#131, user_state#132], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                     +- *(1) ColumnarToRow\n",
      "                        +- FileScan parquet default.userstbl[uid#129,login#130,email#131,user_state#132] Batched: true, Bucketed: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/yschoi/Library/CloudStorage/Dropbox/yunseo/development/BOA..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<uid:string,login:string,email:string,user_state:string>, SelectedBucketsCount: 8 out of 8\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinUsersOrdersBucketDF.explain()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
