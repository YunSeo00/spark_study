{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, desc"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 머신러닝 파이프라인 설계"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/01 17:43:15 WARN Utils: Your hostname, choeyunseoui-MacBookAir.local resolves to a loopback address: 127.0.0.1; using 172.30.34.243 instead (on interface en0)\n",
      "23/04/01 17:43:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/01 17:43:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# SparkSession 생성\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"SparkMllibExampleApp\")\n",
    "         .getOrCreate())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|neighbourhood_cleansed|      room_type|bedrooms|bathrooms|number_of_reviews|price|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|      Western Addition|Entire home/apt|     1.0|      1.0|            180.0|170.0|\n",
      "|        Bernal Heights|Entire home/apt|     2.0|      1.0|            111.0|235.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|             17.0| 65.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|              8.0| 65.0|\n",
      "|      Western Addition|Entire home/apt|     2.0|      1.5|             27.0|785.0|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "airbnbDF = spark.read.parquet(\"../data/sf-airbnb-clean.parquet/\")\n",
    "airbnbDF.select(\"neighbourhood_cleansed\", 'room_type', 'bedrooms', 'bathrooms', 'number_of_reviews', 'price').show(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/01 17:43:44 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5780 rows in the training set, and 1366 in the test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 훈련 및 테스트 데이터 분리\n",
    "trainDF, testDF = airbnbDF.randomSplit([.8, .2], seed = 42)\n",
    "print(f\"\"\"There are {trainDF.count()} rows in the training set, and {testDF.count()} in the test set\"\"\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+\n",
      "|bedrooms|features|price|\n",
      "+--------+--------+-----+\n",
      "|     1.0|   [1.0]|200.0|\n",
      "|     1.0|   [1.0]|130.0|\n",
      "|     1.0|   [1.0]| 95.0|\n",
      "|     1.0|   [1.0]|250.0|\n",
      "|     3.0|   [3.0]|250.0|\n",
      "|     1.0|   [1.0]|115.0|\n",
      "|     1.0|   [1.0]|105.0|\n",
      "|     1.0|   [1.0]| 86.0|\n",
      "|     1.0|   [1.0]|100.0|\n",
      "|     2.0|   [2.0]|220.0|\n",
      "+--------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# VectorAssembler를 이용한 데이터 준비\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "vecAssembler = VectorAssembler(inputCols = [\"bedrooms\"], outputCol=\"features\")\n",
    "vecTrainDF = vecAssembler.transform(trainDF)\n",
    "vecTrainDF.select(\"bedrooms\", \"features\", \"price\").show(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/01 17:43:56 WARN Instrumentation: [aaff0d4e] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/01 17:43:57 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/04/01 17:43:57 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/01 17:43:57 WARN InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 추정기를 사용하여 모델 구축\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "lrModel = lr.fit(vecTrainDF)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The formula for the linear regression line is price = 123.68*bedrooms + 47.51\n"
     ]
    }
   ],
   "source": [
    "# 회귀계수 출력\n",
    "m = round(lrModel.coefficients[0], 2)\n",
    "b = round(lrModel.intercept, 2)\n",
    "print(f\"\"\"The formula for the linear regression line is price = {m}*bedrooms + {b}\"\"\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/01 17:44:00 WARN Instrumentation: [c941f594] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    }
   ],
   "source": [
    "# 파이프라인 구축 (훈련데이터로 모델 학습)\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = [vecAssembler, lr])\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+------------------+\n",
      "|bedrooms|features|price|        prediction|\n",
      "+--------+--------+-----+------------------+\n",
      "|     1.0|   [1.0]| 85.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 45.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 70.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|128.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|159.0|171.18598011578285|\n",
      "+--------+--------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 파이프라인 구축 (테스트데이터 예측)\n",
    "predDF = pipelineModel.transform(testDF)\n",
    "predDF.select(\"bedrooms\", \"features\", \"price\", \"prediction\").show(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# 범주형 데이터 다루기 : one-hot encoding\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# 변환할 변수 지정 및 변환할 변수 명 생성\n",
    "categoricalCols = [field for (field, dataType) in trainDF.dtypes if dataType == \"string\"]\n",
    "indexOutputCols = [x + \"Index\" for x in categoricalCols]\n",
    "oheOutputCols = [x + \"OHE\" for x in categoricalCols]\n",
    "\n",
    "# StringIndexer\n",
    "stringIndexer = StringIndexer(inputCols=categoricalCols, outputCols=indexOutputCols, handleInvalid = \"skip\")\n",
    "# OneHotEncoder\n",
    "oheEncoder = OneHotEncoder(inputCols=indexOutputCols, outputCols=oheOutputCols)\n",
    "\n",
    "# 연속형 변수 지정\n",
    "numericCols = [field for (field, dataType) in trainDF.dtypes if  ((dataType == \"double\") & (field != \"price\"))]\n",
    "\n",
    "# vectorAssembler\n",
    "assemblerInputs = oheOutputCols + numericCols\n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# RFormula 사용하기\n",
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "rFormula = RFormula(formula = \"price ~ .\",\n",
    "                    featuresCol=\"features\",\n",
    "                    labelCol=\"price\",\n",
    "                    handleInvalid=\"skip\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/01 17:44:07 WARN Instrumentation: [5604f60d] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features|price|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|(98,[0,3,6,22,43,...| 85.0| 55.24365707389188|\n",
      "|(98,[0,3,6,22,43,...| 45.0|23.357685914717877|\n",
      "|(98,[0,3,6,22,43,...| 70.0|28.474464479034395|\n",
      "|(98,[0,3,6,12,42,...|128.0| -91.6079079594947|\n",
      "|(98,[0,3,6,12,43,...|159.0| 95.05688229945372|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 범주형 데이터 처리를 포함한 파이프라인 설계\n",
    "lr = LinearRegression(labelCol=\"price\", featuresCol=\"features\")\n",
    "pipeline=Pipeline(stages = [stringIndexer, oheEncoder, vecAssembler, lr])\n",
    "\n",
    "# RFormula\n",
    "# pipeline=Pipeline(stages=[rFormula, lr])\n",
    "\n",
    "pipelineModel = pipeline.fit(trainDF)\n",
    "predDF = pipelineModel.transform(testDF)\n",
    "predDF.select(\"features\", \"price\", \"prediction\").show(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE if 220.6\n"
     ]
    }
   ],
   "source": [
    "# RMSE 측정\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "regressionEvaluator = RegressionEvaluator(\n",
    "    predictionCol = \"prediction\", # 예측값\n",
    "    labelCol = \"price\", # 실제값\n",
    "    metricName=\"rmse\")\n",
    "rmse = regressionEvaluator.evaluate(predDF)\n",
    "print(f\"RMSE if {rmse:.1f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 is 0.16043316698848087\n"
     ]
    }
   ],
   "source": [
    "# R-square\n",
    "r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
    "print(f\"R2 is {r2}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "# pipelineModel.write().overwrite().save(\"저장경로\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# 모델 로드\n",
    "# from pyspark.ml import PipelineModel\n",
    "# savedPipelineModel = PipelineModel.load(\"저장경로\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 하이퍼파라미터 튜닝\n",
    "- 트리 기반 모델"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "# 모델 객체 생성\n",
    "dt = DecisionTreeRegressor(labelCol=\"price\")\n",
    "\n",
    "# 숫자 열만 필터링\n",
    "numericCols = [field for (field, dataType) in trainDF.dtypes\n",
    "               if ((dataType == \"double\") & (field != \"price\"))]\n",
    "\n",
    "# 위에서 정의한 StingIndexer의 출력과 숫자 열 결합\n",
    "assemblerInputs = indexOutputCols + numericCols\n",
    "vecAssembler = VectorAssembler(inputCols = assemblerInputs, outputCol=\"features\")\n",
    "\n",
    "# 단계를 파이프라인으로 결합\n",
    "stages = [stringIndexer, vecAssembler, dt]\n",
    "pipeline = Pipeline(stages = stages)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/01 17:44:16 ERROR Instrumentation: java.lang.IllegalArgumentException: requirement failed: DecisionTree requires maxBins (= 32) to be at least as large as the number of values in each categorical feature, but categorical feature 3 has 36 values. Consider removing this and other categorical features with a large number of values, or add more training examples.\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:151)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\n",
      "\tat org.apache.spark.ml.regression.DecisionTreeRegressor.$anonfun$train$1(DecisionTreeRegressor.scala:126)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:114)\n",
      "\tat org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:44)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: DecisionTree requires maxBins (= 32) to be at least as large as the number of values in each categorical feature, but categorical feature 3 has 36 values. Consider removing this and other categorical features with a large number of values, or add more training examples.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/wl/5c7g6f3s5ps1gyg84qj_5n040000gn/T/ipykernel_54175/2691158171.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# 및 훈련 -> error\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mpipelineModel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpipeline\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrainDF\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/Library/CloudStorage/Dropbox/yunseo/development/BOAZ/spark-study/spark_exam_code/spark_study/venv/lib/python3.7/site-packages/pyspark/ml/base.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    203\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 205\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    206\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    207\u001B[0m             raise TypeError(\n",
      "\u001B[0;32m~/Library/CloudStorage/Dropbox/yunseo/development/BOAZ/spark-study/spark_exam_code/spark_study/venv/lib/python3.7/site-packages/pyspark/ml/pipeline.py\u001B[0m in \u001B[0;36m_fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    132\u001B[0m                     \u001B[0mdataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstage\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    133\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# must be an Estimator\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 134\u001B[0;31m                     \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstage\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    135\u001B[0m                     \u001B[0mtransformers\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    136\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0mi\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0mindexOfLastEstimator\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/CloudStorage/Dropbox/yunseo/development/BOAZ/spark-study/spark_exam_code/spark_study/venv/lib/python3.7/site-packages/pyspark/ml/base.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    203\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 205\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    206\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    207\u001B[0m             raise TypeError(\n",
      "\u001B[0;32m~/Library/CloudStorage/Dropbox/yunseo/development/BOAZ/spark-study/spark_exam_code/spark_study/venv/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    381\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    382\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mJM\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 383\u001B[0;31m         \u001B[0mjava_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit_java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    384\u001B[0m         \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjava_model\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    385\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_copyValues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/CloudStorage/Dropbox/yunseo/development/BOAZ/spark-study/spark_exam_code/spark_study/venv/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_fit_java\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    378\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    379\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_transfer_params_to_java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 380\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_java_obj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    381\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    382\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mJM\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/CloudStorage/Dropbox/yunseo/development/BOAZ/spark-study/spark_exam_code/spark_study/venv/lib/python3.7/site-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1321\u001B[0m         return_value = get_return_value(\n\u001B[0;32m-> 1322\u001B[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0m\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1324\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mtemp_arg\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/CloudStorage/Dropbox/yunseo/development/BOAZ/spark-study/spark_exam_code/spark_study/venv/lib/python3.7/site-packages/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    194\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    195\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 196\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    197\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    198\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mIllegalArgumentException\u001B[0m: requirement failed: DecisionTree requires maxBins (= 32) to be at least as large as the number of values in each categorical feature, but categorical feature 3 has 36 values. Consider removing this and other categorical features with a large number of values, or add more training examples."
     ]
    }
   ],
   "source": [
    "# 및 훈련 -> error\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dt.setMaxBins(40)\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeRegressionModel: uid=DecisionTreeRegressor_9ac4c456a13f, depth=5, numNodes=47, numFeatures=33\n",
      "  If (feature 12 <= 2.5)\n",
      "   If (feature 12 <= 1.5)\n",
      "    If (feature 5 in {1.0,2.0})\n",
      "     If (feature 4 in {0.0,1.0,3.0,5.0,9.0,10.0,11.0,13.0,14.0,16.0,18.0,24.0})\n",
      "      If (feature 3 in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0})\n",
      "       Predict: 104.23992784125075\n",
      "      Else (feature 3 not in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0})\n",
      "       Predict: 250.7111111111111\n",
      "     Else (feature 4 not in {0.0,1.0,3.0,5.0,9.0,10.0,11.0,13.0,14.0,16.0,18.0,24.0})\n",
      "      If (feature 3 in {0.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,27.0,33.0,35.0})\n",
      "       Predict: 151.94179894179894\n",
      "      Else (feature 3 not in {0.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,27.0,33.0,35.0})\n",
      "       Predict: 245.8507462686567\n",
      "    Else (feature 5 not in {1.0,2.0})\n",
      "     If (feature 3 in {1.0,5.0,6.0,7.0,8.0,9.0,11.0,13.0,15.0,16.0,17.0,19.0,22.0,23.0,24.0,25.0,26.0,28.0,29.0,30.0,33.0})\n",
      "      If (feature 3 in {5.0,8.0,13.0,15.0,16.0,19.0,22.0,23.0,24.0,25.0,28.0,30.0,33.0})\n",
      "       Predict: 131.96658097686375\n",
      "      Else (feature 3 not in {5.0,8.0,13.0,15.0,16.0,19.0,22.0,23.0,24.0,25.0,28.0,30.0,33.0})\n",
      "       Predict: 164.19959266802445\n",
      "     Else (feature 3 not in {1.0,5.0,6.0,7.0,8.0,9.0,11.0,13.0,15.0,16.0,17.0,19.0,22.0,23.0,24.0,25.0,26.0,28.0,29.0,30.0,33.0})\n",
      "      If (feature 10 <= 6.5)\n",
      "       Predict: 205.5814889336016\n",
      "      Else (feature 10 > 6.5)\n",
      "       Predict: 841.6666666666666\n",
      "   Else (feature 12 > 1.5)\n",
      "    If (feature 13 <= 4.5)\n",
      "     If (feature 3 in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,15.0,16.0,17.0,18.0,19.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,33.0,34.0})\n",
      "      If (feature 14 <= 26.5)\n",
      "       Predict: 290.8357933579336\n",
      "      Else (feature 14 > 26.5)\n",
      "       Predict: 214.04819277108433\n",
      "     Else (feature 3 not in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,15.0,16.0,17.0,18.0,19.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,33.0,34.0})\n",
      "      If (feature 14 <= 3.5)\n",
      "       Predict: 741.64\n",
      "      Else (feature 14 > 3.5)\n",
      "       Predict: 309.03921568627453\n",
      "    Else (feature 13 > 4.5)\n",
      "     If (feature 15 <= 0.5)\n",
      "      If (feature 2 in {1.0})\n",
      "       Predict: 300.0\n",
      "      Else (feature 2 not in {1.0})\n",
      "       Predict: 10000.0\n",
      "     Else (feature 15 > 0.5)\n",
      "      If (feature 3 in {1.0,4.0,5.0,7.0,8.0,19.0})\n",
      "       Predict: 222.91666666666666\n",
      "      Else (feature 3 not in {1.0,4.0,5.0,7.0,8.0,19.0})\n",
      "       Predict: 398.0\n",
      "  Else (feature 12 > 2.5)\n",
      "   If (feature 1 in {0.0,1.0,2.0,3.0,4.0})\n",
      "    If (feature 12 <= 5.5)\n",
      "     If (feature 3 in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,10.0,11.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,21.0,22.0,23.0,24.0,25.0,26.0,28.0,29.0,30.0,33.0})\n",
      "      If (feature 14 <= 7.5)\n",
      "       Predict: 493.3795620437956\n",
      "      Else (feature 14 > 7.5)\n",
      "       Predict: 296.76666666666665\n",
      "     Else (feature 3 not in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,10.0,11.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,21.0,22.0,23.0,24.0,25.0,26.0,28.0,29.0,30.0,33.0})\n",
      "      If (feature 9 <= -122.411075)\n",
      "       Predict: 722.96875\n",
      "      Else (feature 9 > -122.411075)\n",
      "       Predict: 2399.4\n",
      "    Else (feature 12 > 5.5)\n",
      "     If (feature 4 in {0.0,1.0,5.0,7.0})\n",
      "      If (feature 3 in {0.0,3.0,6.0,25.0})\n",
      "       Predict: 609.5\n",
      "      Else (feature 3 not in {0.0,3.0,6.0,25.0})\n",
      "       Predict: 1715.0\n",
      "     Else (feature 4 not in {0.0,1.0,5.0,7.0})\n",
      "      Predict: 8000.0\n",
      "   Else (feature 1 not in {0.0,1.0,2.0,3.0,4.0})\n",
      "    Predict: 8000.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 규칙 프린트\n",
    "dtModel = pipelineModel.stages[-1]\n",
    "print(dtModel.toDebugString)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "                           feature  importance\n12                        bedrooms    0.283406\n1         cancellation_policyIndex    0.167893\n2            instant_bookableIndex    0.140081\n4               property_typeIndex    0.128179\n15               number_of_reviews    0.126233\n3      neighbourhood_cleansedIndex    0.056200\n9                        longitude    0.038810\n14                  minimum_nights    0.029473\n13                            beds    0.015218\n5                   room_typeIndex    0.010905\n10                    accommodates    0.003603\n0           host_is_superhostIndex    0.000000\n24                    bathrooms_na    0.000000\n25                         beds_na    0.000000\n26         review_scores_rating_na    0.000000\n27       review_scores_accuracy_na    0.000000\n28    review_scores_cleanliness_na    0.000000\n22             review_scores_value    0.000000\n29        review_scores_checkin_na    0.000000\n30  review_scores_communication_na    0.000000\n31       review_scores_location_na    0.000000\n23                     bedrooms_na    0.000000\n16            review_scores_rating    0.000000\n21          review_scores_location    0.000000\n20     review_scores_communication    0.000000\n19           review_scores_checkin    0.000000\n18       review_scores_cleanliness    0.000000\n17          review_scores_accuracy    0.000000\n11                       bathrooms    0.000000\n8                         latitude    0.000000\n7        host_total_listings_count    0.000000\n6                    bed_typeIndex    0.000000\n32          review_scores_value_na    0.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature</th>\n      <th>importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12</th>\n      <td>bedrooms</td>\n      <td>0.283406</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cancellation_policyIndex</td>\n      <td>0.167893</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>instant_bookableIndex</td>\n      <td>0.140081</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>property_typeIndex</td>\n      <td>0.128179</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>number_of_reviews</td>\n      <td>0.126233</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>neighbourhood_cleansedIndex</td>\n      <td>0.056200</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>longitude</td>\n      <td>0.038810</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>minimum_nights</td>\n      <td>0.029473</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>beds</td>\n      <td>0.015218</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>room_typeIndex</td>\n      <td>0.010905</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>accommodates</td>\n      <td>0.003603</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>host_is_superhostIndex</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>bathrooms_na</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>beds_na</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>review_scores_rating_na</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>review_scores_accuracy_na</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>review_scores_cleanliness_na</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>review_scores_value</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>review_scores_checkin_na</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>review_scores_communication_na</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>review_scores_location_na</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>bedrooms_na</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>review_scores_rating</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>review_scores_location</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>review_scores_communication</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>review_scores_checkin</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>review_scores_cleanliness</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>review_scores_accuracy</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>bathrooms</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>latitude</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>host_total_listings_count</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>bed_typeIndex</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>review_scores_value_na</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 변수 중요도 출력\n",
    "import pandas as pd\n",
    "\n",
    "featureImp = pd.DataFrame(\n",
    "    list(zip(vecAssembler.getInputCols(), dtModel.featureImportances)),\n",
    "    columns = [\"feature\", \"importance\"]\n",
    ")\n",
    "featureImp.sort_values(by = \"importance\", ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# 랜덤포레스트\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rf = RandomForestRegressor(labelCol=\"price\", maxBins=40, seed = 42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### K-폴드 교차검증"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "pipeline = Pipeline(stages = [stringIndexer, vecAssembler, rf])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# ParamGridBuilder()\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, [2,4,6])\n",
    "             .addGrid(rf.numTrees, [10, 100])\n",
    "             .build())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# metric\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\",\n",
    "                                predictionCol=\"prediction\",\n",
    "                                metricName=\"rmse\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/01 17:51:13 WARN DAGScheduler: Broadcasting large task binary with size 1303.2 KiB\n",
      "23/04/01 17:51:32 WARN DAGScheduler: Broadcasting large task binary with size 1136.8 KiB\n",
      "23/04/01 17:51:49 WARN DAGScheduler: Broadcasting large task binary with size 1177.6 KiB\n",
      "23/04/01 17:51:54 WARN DAGScheduler: Broadcasting large task binary with size 1202.5 KiB\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "cv = CrossValidator(estimator=pipeline,\n",
    "                    evaluator=evaluator,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    numFolds=3,\n",
    "                    seed=42)\n",
    "cvModel = cv.fit(trainDF)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "  host_is_superhost cancellation_policy instant_bookable  \\\n0                 f            flexible                f   \n1                 f            flexible                f   \n2                 f            flexible                f   \n3                 f            flexible                f   \n4                 f            flexible                f   \n\n   host_total_listings_count neighbourhood_cleansed  latitude  longitude  \\\n0                        1.0                Bayview  37.72001 -122.39249   \n1                        1.0                Bayview  37.73250 -122.39221   \n2                        1.0                Bayview  37.73555 -122.39779   \n3                        1.0         Bernal Heights  37.73905 -122.41269   \n4                        1.0         Bernal Heights  37.74473 -122.41516   \n\n  property_type        room_type  accommodates  ...  review_scores_value_na  \\\n0         House  Entire home/apt           2.0  ...                     0.0   \n1         House     Private room           1.0  ...                     1.0   \n2         House     Private room           1.0  ...                     1.0   \n3     Apartment     Private room           1.0  ...                     0.0   \n4         House     Private room           1.0  ...                     0.0   \n\n   host_is_superhostIndex  cancellation_policyIndex instant_bookableIndex  \\\n0                     0.0                       2.0                   0.0   \n1                     0.0                       2.0                   0.0   \n2                     0.0                       2.0                   0.0   \n3                     0.0                       2.0                   0.0   \n4                     0.0                       2.0                   0.0   \n\n   neighbourhood_cleansedIndex  property_typeIndex  room_typeIndex  \\\n0                         15.0                 1.0             0.0   \n1                         15.0                 1.0             1.0   \n2                         15.0                 1.0             1.0   \n3                          5.0                 0.0             1.0   \n4                          5.0                 1.0             1.0   \n\n   bed_typeIndex                                           features  \\\n0            0.0  (0.0, 2.0, 0.0, 15.0, 1.0, 0.0, 0.0, 1.0, 37.7...   \n1            0.0  [0.0, 2.0, 0.0, 15.0, 1.0, 1.0, 0.0, 1.0, 37.7...   \n2            0.0  [0.0, 2.0, 0.0, 15.0, 1.0, 1.0, 0.0, 1.0, 37.7...   \n3            0.0  (0.0, 2.0, 0.0, 5.0, 0.0, 1.0, 0.0, 1.0, 37.73...   \n4            0.0  (0.0, 2.0, 0.0, 5.0, 1.0, 1.0, 0.0, 1.0, 37.74...   \n\n   prediction  \n0  143.861264  \n1   90.568314  \n2   91.016124  \n3   93.106212  \n4  113.129252  \n\n[5 rows x 43 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>host_is_superhost</th>\n      <th>cancellation_policy</th>\n      <th>instant_bookable</th>\n      <th>host_total_listings_count</th>\n      <th>neighbourhood_cleansed</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>property_type</th>\n      <th>room_type</th>\n      <th>accommodates</th>\n      <th>...</th>\n      <th>review_scores_value_na</th>\n      <th>host_is_superhostIndex</th>\n      <th>cancellation_policyIndex</th>\n      <th>instant_bookableIndex</th>\n      <th>neighbourhood_cleansedIndex</th>\n      <th>property_typeIndex</th>\n      <th>room_typeIndex</th>\n      <th>bed_typeIndex</th>\n      <th>features</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>f</td>\n      <td>flexible</td>\n      <td>f</td>\n      <td>1.0</td>\n      <td>Bayview</td>\n      <td>37.72001</td>\n      <td>-122.39249</td>\n      <td>House</td>\n      <td>Entire home/apt</td>\n      <td>2.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>15.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>(0.0, 2.0, 0.0, 15.0, 1.0, 0.0, 0.0, 1.0, 37.7...</td>\n      <td>143.861264</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>f</td>\n      <td>flexible</td>\n      <td>f</td>\n      <td>1.0</td>\n      <td>Bayview</td>\n      <td>37.73250</td>\n      <td>-122.39221</td>\n      <td>House</td>\n      <td>Private room</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>15.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>[0.0, 2.0, 0.0, 15.0, 1.0, 1.0, 0.0, 1.0, 37.7...</td>\n      <td>90.568314</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>f</td>\n      <td>flexible</td>\n      <td>f</td>\n      <td>1.0</td>\n      <td>Bayview</td>\n      <td>37.73555</td>\n      <td>-122.39779</td>\n      <td>House</td>\n      <td>Private room</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>15.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>[0.0, 2.0, 0.0, 15.0, 1.0, 1.0, 0.0, 1.0, 37.7...</td>\n      <td>91.016124</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>f</td>\n      <td>flexible</td>\n      <td>f</td>\n      <td>1.0</td>\n      <td>Bernal Heights</td>\n      <td>37.73905</td>\n      <td>-122.41269</td>\n      <td>Apartment</td>\n      <td>Private room</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>(0.0, 2.0, 0.0, 5.0, 0.0, 1.0, 0.0, 1.0, 37.73...</td>\n      <td>93.106212</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>f</td>\n      <td>flexible</td>\n      <td>f</td>\n      <td>1.0</td>\n      <td>Bernal Heights</td>\n      <td>37.74473</td>\n      <td>-122.41516</td>\n      <td>House</td>\n      <td>Private room</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>(0.0, 2.0, 0.0, 5.0, 1.0, 1.0, 0.0, 1.0, 37.74...</td>\n      <td>113.129252</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 43 columns</p>\n</div>"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.transform(testDF).toPandas().head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
