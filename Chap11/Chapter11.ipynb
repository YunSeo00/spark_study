{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MLflow"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/01 16:06:54 WARN Utils: Your hostname, choeyunseoui-MacBookAir.local resolves to a loopback address: 127.0.0.1; using 172.30.34.243 instead (on interface en0)\n",
      "23/04/01 16:06:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/01 16:06:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# SparkSession 생성\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"SparkMllibExampleApp\")\n",
    "         .getOrCreate())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### randomforest model mlflow로 트래킹하기"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filePath = \"../data/sf-airbnb-clean.parquet\"\n",
    "airbnbDF = spark.read.parquet(filePath)\n",
    "(trainDF, testDF) = airbnbDF.randomSplit([.8, .2], seed = 42)\n",
    "\n",
    "categoricalCols = [field for (field, dataType) in trainDF.dtypes if dataType == \"string\"]\n",
    "indexOutputCols = [x + \"Index\" for x in categoricalCols]\n",
    "stringIndexer = StringIndexer(inputCols = categoricalCols,\n",
    "                              outputCols = indexOutputCols,\n",
    "                              handleInvalid = \"skip\")\n",
    "numericCols = [field for (field, dataType) in trainDF.dtypes if ((dataType == \"double\") & (field != \"price\"))]\n",
    "assemblerInputs = indexOutputCols + numericCols\n",
    "vecAssembler = VectorAssembler(inputCols = assemblerInputs,\n",
    "                               outputCol = \"features\")\n",
    "\n",
    "rf = RandomForestRegressor(labelCol = \"price\", maxBins = 40, maxDepth = 5, numTrees = 100, seed = 42)\n",
    "\n",
    "pipeline = Pipeline(stages = [stringIndexer, vecAssembler, rf])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/01 16:07:07 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yschoi/Library/CloudStorage/Dropbox/yunseo/development/BOAZ/spark-study/spark_exam_code/spark_study/venv/lib/python3.7/site-packages/_distutils_hack/__init__.py:36: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name = \"random-forest\") as run:\n",
    "    # 로그 매개변수\n",
    "    mlflow.log_param(\"num_trees\", rf.getNumTrees())\n",
    "    mlflow.log_param(\"max_depth\", rf.getMaxDepth())\n",
    "\n",
    "    # 로그 모델\n",
    "    pipelineModel = pipeline.fit(trainDF)\n",
    "    mlflow.spark.log_model(pipelineModel, \"model\")\n",
    "\n",
    "    # 로그 매트릭\n",
    "    predDF = pipelineModel.transform(testDF)\n",
    "    regressionEvaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\")\n",
    "    rmse = regressionEvaluator.setMetricName(\"rmse\").evaluate(predDF)\n",
    "    r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
    "    mlflow.log_metrics({\"rmse\": rmse, \"r2\": r2})\n",
    "\n",
    "    # 로그 아티팩트(artifact) : 기능 중요도 점수\n",
    "    rfModel = pipelineModel.stages[-1]\n",
    "    pandasDF = (pd.DataFrame(list(zip(vecAssembler.getInputCols(),\n",
    "                                      rfModel.featureImportances)),\n",
    "                             columns=[\"feature\", \"importance\"])\n",
    "                .sort_values(by = \"importance\", ascending = False))\n",
    "\n",
    "    # 먼저 로컬에 파일을 저장하고 MLflow에 파일 경로를 알려준다.\n",
    "    pandasDF.to_csv(\"feature-importance.csv\", index = False)\n",
    "    mlflow.log_artifact(\"feature-importance.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```\n",
    "cd (mlruns 폴더가 있는 폴더로 이동)\n",
    "mlflow ui\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "MLflowClient로 트래킹 파일 접근"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443c46fd533e4bfdb33dd3efe0372c2b\n",
      "random-forest\n",
      "{'r2': 0.22794251914574226, 'rmse': 211.5096898777315}\n"
     ]
    }
   ],
   "source": [
    "client = MlflowClient()\n",
    "runs = client.search_runs(run.info.experiment_id,\n",
    "                         order_by= [\"attributes.start_time desc\"],\n",
    "                          max_results=3) # max_results : 몇 개까지 찾을 건지\n",
    "run_id = runs[2].info.run_id\n",
    "run_name = runs[2].info.run_name\n",
    "print(run_id)\n",
    "print(run_name)\n",
    "print(runs[2].data.metrics)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "배치 배포"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/04/01 17:04:18 INFO mlflow.spark: 'runs:/443c46fd533e4bfdb33dd3efe0372c2b/model' resolved as 'file:///Users/yschoi/Library/CloudStorage/Dropbox/yunseo/development/BOAZ/spark-study/spark_exam_code/spark_study/Chap11/mlruns/0/443c46fd533e4bfdb33dd3efe0372c2b/artifacts/model'\n",
      "2023/04/01 17:04:18 INFO mlflow.spark: File 'file:///Users/yschoi/Library/CloudStorage/Dropbox/yunseo/development/BOAZ/spark-study/spark_exam_code/spark_study/Chap11/mlruns/0/443c46fd533e4bfdb33dd3efe0372c2b/artifacts/model/sparkml' is already on DFS, copy is not necessary.\n"
     ]
    }
   ],
   "source": [
    "# mlflow를 사용하여 저장된 모델 로드\n",
    "pipelineModel = mlflow.spark.load_model(f\"runs:/{run_id}/model\")\n",
    "# 예측 생성\n",
    "inputDF = spark.read.parquet(\"../data/sf-airbnb-clean.parquet\")\n",
    "predDF = pipelineModel.transform(inputDF)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "  host_is_superhost          cancellation_policy instant_bookable  \\\n0                 t                     moderate                t   \n1                 f  strict_14_with_grace_period                f   \n2                 f  strict_14_with_grace_period                f   \n3                 f  strict_14_with_grace_period                f   \n4                 f  strict_14_with_grace_period                f   \n\n   host_total_listings_count neighbourhood_cleansed  latitude  longitude  \\\n0                        1.0       Western Addition  37.76931 -122.43386   \n1                        2.0         Bernal Heights  37.74511 -122.42102   \n2                       10.0         Haight Ashbury  37.76669 -122.45250   \n3                       10.0         Haight Ashbury  37.76487 -122.45183   \n4                        2.0       Western Addition  37.77525 -122.43637   \n\n  property_type        room_type  accommodates  ...  review_scores_value_na  \\\n0     Apartment  Entire home/apt           3.0  ...                     0.0   \n1     Apartment  Entire home/apt           5.0  ...                     0.0   \n2     Apartment     Private room           2.0  ...                     0.0   \n3     Apartment     Private room           2.0  ...                     0.0   \n4         House  Entire home/apt           5.0  ...                     0.0   \n\n   host_is_superhostIndex  cancellation_policyIndex instant_bookableIndex  \\\n0                     1.0                       1.0                   1.0   \n1                     0.0                       0.0                   0.0   \n2                     0.0                       0.0                   0.0   \n3                     0.0                       0.0                   0.0   \n4                     0.0                       0.0                   0.0   \n\n   neighbourhood_cleansedIndex  property_typeIndex  room_typeIndex  \\\n0                          1.0                 0.0             0.0   \n1                          5.0                 0.0             0.0   \n2                          6.0                 0.0             1.0   \n3                          6.0                 0.0             1.0   \n4                          1.0                 1.0             0.0   \n\n   bed_typeIndex                                           features  \\\n0            0.0  (1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 37.76...   \n1            0.0  (0.0, 0.0, 0.0, 5.0, 0.0, 0.0, 0.0, 2.0, 37.74...   \n2            0.0  (0.0, 0.0, 0.0, 6.0, 0.0, 1.0, 0.0, 10.0, 37.7...   \n3            0.0  (0.0, 0.0, 0.0, 6.0, 0.0, 1.0, 0.0, 10.0, 37.7...   \n4            0.0  (0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 2.0, 37.77...   \n\n   prediction  \n0  175.772744  \n1  237.216472  \n2  103.335423  \n3  103.838724  \n4  302.503672  \n\n[5 rows x 43 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>host_is_superhost</th>\n      <th>cancellation_policy</th>\n      <th>instant_bookable</th>\n      <th>host_total_listings_count</th>\n      <th>neighbourhood_cleansed</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>property_type</th>\n      <th>room_type</th>\n      <th>accommodates</th>\n      <th>...</th>\n      <th>review_scores_value_na</th>\n      <th>host_is_superhostIndex</th>\n      <th>cancellation_policyIndex</th>\n      <th>instant_bookableIndex</th>\n      <th>neighbourhood_cleansedIndex</th>\n      <th>property_typeIndex</th>\n      <th>room_typeIndex</th>\n      <th>bed_typeIndex</th>\n      <th>features</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>t</td>\n      <td>moderate</td>\n      <td>t</td>\n      <td>1.0</td>\n      <td>Western Addition</td>\n      <td>37.76931</td>\n      <td>-122.43386</td>\n      <td>Apartment</td>\n      <td>Entire home/apt</td>\n      <td>3.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>(1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 37.76...</td>\n      <td>175.772744</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>f</td>\n      <td>strict_14_with_grace_period</td>\n      <td>f</td>\n      <td>2.0</td>\n      <td>Bernal Heights</td>\n      <td>37.74511</td>\n      <td>-122.42102</td>\n      <td>Apartment</td>\n      <td>Entire home/apt</td>\n      <td>5.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>(0.0, 0.0, 0.0, 5.0, 0.0, 0.0, 0.0, 2.0, 37.74...</td>\n      <td>237.216472</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>f</td>\n      <td>strict_14_with_grace_period</td>\n      <td>f</td>\n      <td>10.0</td>\n      <td>Haight Ashbury</td>\n      <td>37.76669</td>\n      <td>-122.45250</td>\n      <td>Apartment</td>\n      <td>Private room</td>\n      <td>2.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>(0.0, 0.0, 0.0, 6.0, 0.0, 1.0, 0.0, 10.0, 37.7...</td>\n      <td>103.335423</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>f</td>\n      <td>strict_14_with_grace_period</td>\n      <td>f</td>\n      <td>10.0</td>\n      <td>Haight Ashbury</td>\n      <td>37.76487</td>\n      <td>-122.45183</td>\n      <td>Apartment</td>\n      <td>Private room</td>\n      <td>2.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>(0.0, 0.0, 0.0, 6.0, 0.0, 1.0, 0.0, 10.0, 37.7...</td>\n      <td>103.838724</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>f</td>\n      <td>strict_14_with_grace_period</td>\n      <td>f</td>\n      <td>2.0</td>\n      <td>Western Addition</td>\n      <td>37.77525</td>\n      <td>-122.43637</td>\n      <td>House</td>\n      <td>Entire home/apt</td>\n      <td>5.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>(0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 2.0, 37.77...</td>\n      <td>302.503672</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 43 columns</p>\n</div>"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predDF.toPandas().head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "스트리밍 배포"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/04/01 17:32:05 INFO mlflow.spark: 'runs:/443c46fd533e4bfdb33dd3efe0372c2b/model' resolved as 'file:///Users/yschoi/Library/CloudStorage/Dropbox/yunseo/development/BOAZ/spark-study/spark_exam_code/spark_study/Chap11/mlruns/0/443c46fd533e4bfdb33dd3efe0372c2b/artifacts/model'\n",
      "2023/04/01 17:32:05 INFO mlflow.spark: File 'file:///Users/yschoi/Library/CloudStorage/Dropbox/yunseo/development/BOAZ/spark-study/spark_exam_code/spark_study/Chap11/mlruns/0/443c46fd533e4bfdb33dd3efe0372c2b/artifacts/model/sparkml' is already on DFS, copy is not necessary.\n"
     ]
    }
   ],
   "source": [
    "# 모델 로드\n",
    "pipelineModel = mlflow.spark.load_model(f\"runs:/{run_id}/model\")\n",
    "\n",
    "# 스트리밍 데이터 셋업\n",
    "repartitionedPath = \"../data/sf-airbnb-clean-100p.parquet\"\n",
    "schema = spark.read.parquet(repartitionedPath).schema\n",
    "\n",
    "streamingData = (spark\n",
    "                 .readStream\n",
    "                 .schema(schema)\n",
    "                 .option(\"maxFilesPerTrigger\", 1)\n",
    "                 .parquet(repartitionedPath))\n",
    "\n",
    "# 예측 생성\n",
    "streamPred = pipelineModel.transform(streamingData)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "#streamPred.writeStream.format(\"console\").start().awaitTermination()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 비 MLlib 모델에 스파크 활용"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/05 15:09:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# SparkSession 생성\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"learn pandas UDFs in Spark\")\n",
    "         .getOrCreate())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# sample data 생성\n",
    "g = np.tile(['group a', 'group b'], 10)\n",
    "x = np.linspace(0, 10., 20)\n",
    "np.random.seed(3)\n",
    "y_lin = 2*x + np.random.rand(len(x))/10.\n",
    "y_qua = 3*x**2 + np.random.rand(len(x))\n",
    "df = pd.DataFrame({'group': g, 'x':x, 'y_lin': y_lin, 'y_qua':y_qua})\n",
    "schema = \"group STRING, x DOUBLE, y_lin DOUBLE, y_qua DOUBLE\"\n",
    "\n",
    "df = spark.createDataFrame(df, schema=schema)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+-------------------+\n",
      "|  group|                 x|              y_lin|              y_qua|\n",
      "+-------+------------------+-------------------+-------------------+\n",
      "|group a|               0.0|0.05507979025745755|0.28352508177131874|\n",
      "|group b|0.5263157894736842|  1.123446361209179| 1.5241628490609185|\n",
      "|group a|1.0526315789473684|  2.134353631786031| 3.7645534406624286|\n",
      "+-------+------------------+-------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|             result|\n",
      "+-------------------+\n",
      "|-0.7071067811865475|\n",
      "| 0.7071067811865475|\n",
      "|-0.7071067811865472|\n",
      "+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 표준화 작업 : series to series pandas UDF\n",
    "@F.pandas_udf(T.DoubleType())\n",
    "def standardise(col1: pd.Series) -> pd.Series:\n",
    "    return (col1 - col1.mean()) / col1.std()\n",
    "\n",
    "res = df.select(standardise(F.col('y_lin')).alias('result')) # alias : column 이름 변경\n",
    "\n",
    "res.show(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# 위와 동일한 기능을 하는 코드입니다.\n",
    "# def standardise_func(col1: pd.Series) -> pd.Series:\n",
    "#     return (col1 - col1.mean()) / col1.std()\n",
    "#\n",
    "# standardise = F.pandas_udf(standardise_func, returnType=T.DoubleType())\n",
    "#\n",
    "# res = df.select(standardise(F.col('y_lin')).alias('result'))\n",
    "# res.show(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|       plus_one(x)|\n",
      "+------------------+\n",
      "|               0.0|\n",
      "|1.0526315789473684|\n",
      "|2.1052631578947367|\n",
      "+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 전체 열에 *2 하기: Iterator of Series to Iterator of Series\n",
    "from typing import Iterator\n",
    "\n",
    "@F.pandas_udf(T.DoubleType())\n",
    "def plus_one(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    for x in iterator:\n",
    "        yield x * 2\n",
    "\n",
    "df.select(plus_one(F.col('x'))).show(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|calculate_rmse(x, y_lin)|\n",
      "+------------------------+\n",
      "|    0.003033783294805516|\n",
      "|    0.005014733386787644|\n",
      "|    8.462556712200734E-4|\n",
      "+------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 두 열의 RMSE 구하기 : Iterator of Multiple Series to Iterator of Series\n",
    "from typing import Tuple\n",
    "\n",
    "@F.pandas_udf(T.DoubleType())\n",
    "def calculate_rmse(iterator: Iterator[Tuple[pd.Series, pd.Series]]) -> Iterator[pd.Series]:\n",
    "    for a, b in iterator:\n",
    "        yield ((a * 2) - b) ** 2\n",
    "\n",
    "df.select(calculate_rmse(\"x\", \"y_lin\")).show(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:=====================>                                    (3 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     group         x     y_lin      y_qua\n",
      "0  group a  4.736842  0.055080   0.283525\n",
      "1  group a  4.736842  2.134354   3.764553\n",
      "2  group a  4.736842  4.299821  13.841048\n",
      "      group         x      y_lin       y_qua\n",
      "17  group b  5.263158  15.848560  187.072826\n",
      "18  group b  5.263158  17.950622  240.724046\n",
      "19  group b  5.263158  20.041510  300.225055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# applyInPandas() : group 별 평균 구하기\n",
    "def subtract_mean(pdf):\n",
    "    # pdf is a pandas.DataFrame\n",
    "    x = pdf.x\n",
    "    return pdf.assign(x = x.mean())\n",
    "\n",
    "result = df.groupby(\"group\").applyInPandas(subtract_mean, schema = \"group STRING, x DOUBLE, y_lin DOUBLE, y_qua DOUBLE\").toPandas()\n",
    "\n",
    "print(result.head(3))\n",
    "print(result.tail(3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4)\n",
      "group a    10\n",
      "Name: group, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# mapInPandas() : group a 데이터만 반환하기\n",
    "def filter_func(iterator):\n",
    "    for pdf in iterator:\n",
    "        yield pdf[pdf.group == \"group a\"]\n",
    "\n",
    "result = df.mapInPandas(filter_func, schema=df.schema).toPandas()\n",
    "\n",
    "print(result.shape)\n",
    "print(result.group.value_counts())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Joblib과 Hyperopt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "from sklearn.utils import parallel_backend\n",
    "from joblibspark import register_spark\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# SparkSession 생성\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"learn pandas UDFs in Spark\")\n",
    "         .getOrCreate())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "register_spark() # 스파크 백엔드 등록"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "filePath = \"../data/sf-airbnb-clean.parquet\"\n",
    "df = spark.read.parquet(filePath).toPandas()\n",
    "df = df.drop(columns = ['host_is_superhost', 'cancellation_policy', 'instant_bookable', 'neighbourhood_cleansed', 'property_type', 'room_type', 'bed_type']) # 수치형 변수만 남기\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop([\"price\"], axis=1), df[[\"price\"]].values.ravel(), random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5359, 26)\n",
      "(1787, 26)\n",
      "(5359,)\n",
      "(1787,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "param_grid = {\"max_depth\" : [2,5,7,10], \"n_estimators\": [20, 50, 70, 100]}\n",
    "gscv = GridSearchCV(rf, param_grid, cv = 3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실행시간 :  16.254735946655273\n",
      "{'mean_fit_time': array([0.10824704, 0.25440931, 0.2828993 , 0.55776437, 0.18814659,\n",
      "       0.67691731, 0.74300226, 0.94147054, 0.24972892, 0.57117605,\n",
      "       0.78440499, 1.13093535, 0.31270862, 0.77415482, 1.10046101,\n",
      "       1.51572259]), 'std_fit_time': array([0.00554672, 0.00740932, 0.00099481, 0.00452771, 0.0013116 ,\n",
      "       0.00244232, 0.01217439, 0.00153647, 0.00077973, 0.00340503,\n",
      "       0.00252918, 0.00261801, 0.00039822, 0.00121841, 0.00346425,\n",
      "       0.00418205]), 'mean_score_time': array([0.00862328, 0.00863004, 0.01081292, 0.03232431, 0.0101157 ,\n",
      "       0.01667802, 0.01397141, 0.01965793, 0.00889333, 0.01314616,\n",
      "       0.01860468, 0.02376199, 0.00869926, 0.0159595 , 0.02241206,\n",
      "       0.03062542]), 'std_score_time': array([0.00238347, 0.00057516, 0.00053839, 0.01019505, 0.00271928,\n",
      "       0.00085583, 0.00066837, 0.00054107, 0.00022919, 0.00126802,\n",
      "       0.00028632, 0.00066272, 0.00029524, 0.00032214, 0.00048547,\n",
      "       0.00107858]), 'param_max_depth': masked_array(data=[2, 2, 2, 2, 5, 5, 5, 5, 7, 7, 7, 7, 10, 10, 10, 10],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_n_estimators': masked_array(data=[20, 50, 70, 100, 20, 50, 70, 100, 20, 50, 70, 100, 20,\n",
      "                   50, 70, 100],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'max_depth': 2, 'n_estimators': 20}, {'max_depth': 2, 'n_estimators': 50}, {'max_depth': 2, 'n_estimators': 70}, {'max_depth': 2, 'n_estimators': 100}, {'max_depth': 5, 'n_estimators': 20}, {'max_depth': 5, 'n_estimators': 50}, {'max_depth': 5, 'n_estimators': 70}, {'max_depth': 5, 'n_estimators': 100}, {'max_depth': 7, 'n_estimators': 20}, {'max_depth': 7, 'n_estimators': 50}, {'max_depth': 7, 'n_estimators': 70}, {'max_depth': 7, 'n_estimators': 100}, {'max_depth': 10, 'n_estimators': 20}, {'max_depth': 10, 'n_estimators': 50}, {'max_depth': 10, 'n_estimators': 70}, {'max_depth': 10, 'n_estimators': 100}], 'split0_test_score': array([ 0.10058484,  0.10647012,  0.1088581 ,  0.11074081, -0.05017605,\n",
      "       -0.04174908, -0.04321374, -0.02135749, -0.07040541, -0.00468249,\n",
      "       -0.03770858, -0.01039798, -0.04647665, -0.04192022, -0.04537098,\n",
      "       -0.03194478]), 'split1_test_score': array([0.17317594, 0.17373451, 0.17397643, 0.16160481, 0.20490735,\n",
      "       0.19970228, 0.19450595, 0.18085573, 0.19636584, 0.20755671,\n",
      "       0.1985927 , 0.18331427, 0.19723831, 0.22733208, 0.21824953,\n",
      "       0.20911046]), 'split2_test_score': array([0.08269175, 0.08079994, 0.08854388, 0.0887891 , 0.11897724,\n",
      "       0.10053583, 0.11530853, 0.10843128, 0.05616497, 0.09750296,\n",
      "       0.10766349, 0.10772638, 0.0867263 , 0.1149984 , 0.11756991,\n",
      "       0.11372813]), 'mean_test_score': array([0.11881751, 0.12033486, 0.1237928 , 0.12037824, 0.09123618,\n",
      "       0.08616301, 0.08886691, 0.08930984, 0.06070846, 0.10012573,\n",
      "       0.08951587, 0.09354756, 0.07916265, 0.10013675, 0.09681616,\n",
      "       0.0969646 ]), 'std_test_score': array([0.03912518, 0.03918658, 0.03644141, 0.030498  , 0.10596874,\n",
      "       0.09909465, 0.0988333 , 0.08365313, 0.10895628, 0.08666614,\n",
      "       0.09731932, 0.07971569, 0.09963986, 0.11042298, 0.10861854,\n",
      "       0.0991217 ]), 'rank_test_score': array([ 4,  3,  1,  2, 10, 14, 13, 12, 16,  6, 11,  9, 15,  5,  8,  7],\n",
      "      dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "with parallel_backend(\"spark\", n_jobs = 3):\n",
    "    gscv.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"실행시간 : \", end_time - start_time) # 16.25\n",
    "\n",
    "print(gscv.cv_results_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실행시간: 25.99013590812683\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "gscv.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "print(\"실행시간:\", end_time - start_time) # 25.99"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hyperopt (실행X) - 스니펫 코드"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import hyperopt\n",
    "\n",
    "best_hyperparameters = hyperopt.fmin(\n",
    "    fn = training_function, # 모델 훈련 함수\n",
    "    space = search_space, # 하이퍼파라미터 탐색 영역 지정\n",
    "    algo = hyperopt.tpe.suggest, # 최적화 알고리즘 선택\n",
    "    max_evals = 64, # 최대 반복 횟수\n",
    "    trials = hyperopt.SparkTrials(parallelism = 4)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
